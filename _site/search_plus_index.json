{"/mldrop-pages/mldrop/2001-02-05-Simulations.html": {
    "title": "Simulations",
    "keywords": "mldrop",
    "url": "/mldrop-pages/mldrop/2001-02-05-Simulations.html",
    "body": "This part describes the different simulations carried so far (using RMSE as a loss function). U and V First simulation takes as input the velocity field to learn the advection of the probability densities. Here we are learning exactly from the same data that are used to generate the ground-truth. With that being said, we (from AI point of view) expect it to have the best results! We run our experiment for a total of 20 epochs, and the image below shows the evolution of the loss function against each epoch. Our machine converges quite fast but starts over-training right after the 5th Epoch! Video example of the ground truth vs ML(U,V) advection: The densities are too intense! SSH Second simulation takes as input Sea Surface Height to learn the advection of the probability densities. If we consider observational data, SSH should give better results than (U,V). The latter are derived from SSH under geostrophic approximation and for sure admit mapping/processing errors! We run our experiment for a total of 20 epochs, and the image below shows the evolution of the loss function against each epoch. To my understanding it seems like we learn more from SSH than (U,V). Video example of the ground truth vs ML(ssh) advection: SST and SSH SSH alone gives really promising results and physically makes lot of sense. What if we feed more information to our machine, SST for instance, will it be able to learn more? SST encode, in addition to temperature, information about coherent structures, and they themselves inform us about transport barriers that shape the fluid flow. So theoretically we should retrieve more information. The third simulation takes as input Sea Surface Temperature and Sea Surface Height to learn the advection of the probability densities. We run our experiment for a total of 20 epochs, and the image below shows the evolution of the loss function against each epoch. The results are definitely better that (U,V) but not SSH. Video example of the ground truth vs ML(ssh,sst) advection: Visually, combining SST and SSH seems to miss-up with the machine ( due to different order of magnitude –&gt; standardize SST) SST (t,t+1) Fourth simulation takes as input two consecutive Sea Surface Temperature images to learn the advection of the probability densities. This experiment mostly describe the optical flow which describes pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene. Back to observational data, SST does encode sub-mesoscale dynamics (and barrier transport), and combining SST(t,t+1) will definitely inform us about the causality linking them, that is the transport. We run our experiment for a total of 20 epochs, and the image below shows the evolution of the loss function against each epoch. The results are slightly better than the one above(sst,ssh) and are far better from (U,V). There are some promising results and worth trying them on satellite images! Video example of the ground truth vs ML(sst(t,t+1)) advection: Standardized SST (t,t+1) The fifth simulation is similar to the one above but SST are standardized. We run our experiment for a total of 20 epochs, and the image below shows the evolution of the loss function against each epoch. We observe an improvement of the results compared to raw SST! Video example of the ground truth vs ML(Std(sst[t,t+1])) advection: SSH and standardized SST The sixth simulation takes as input standardized Sea Surface Temperature and Sea Surface Height to learn the advection of the probability densities. We run our experiment for a total of 20 epochs, and the image below shows the evolution of the loss function against each epoch. The results are not different from standardized SST alone. Video example of the ground truth vs ML(Std(sst), ssh) advection: Well now that both SST an SSH are around the same range, it looks okay, therefore the difference between ssh/sst ranges was the issue! Intercomparison Sorry but (U,V) ruined pretty much everything! Jump this and use the interactive plot (Select/Unselect variable)"
  },"/mldrop-pages/mldrop/2001-02-04-config.html": {
    "title": "ML-Drop Setup",
    "keywords": "mldrop",
    "url": "/mldrop-pages/mldrop/2001-02-04-config.html",
    "body": "This part describes the setup used in ML-Drop project. Region of study This work focuses on the region between the Balearic islands and the Algerian coastline (encircled bellow in black) The choice of this particular areas is motivated mainly by the CNES-NASA SWOT mission, and more particularly the research activities of MEOM-team. Generation of density maps In this project we are mainly learning the fokker-planck equation of a lost object in the ocean, that is the time-evolution of probability density function of a object released in the ocean, which also can be modeled in the form of an SDE. Here we are talking about three main inputs (Area of the initial release of the particle, Time-length of trajectories and Δt) that are discussed in the following: Area of particle release: We choose the region (span [37.89N ~ 38.89N][ 2.71E ~ 3.71E]) encircled in the image below as the area of particle release, because it will benefit from high SWOT coverage both in space and time. Time-length of trajectories: To chose the optimal advection time, we carried a small experiment where we initialized 500 particles daily and randomly in the area of particle release between 01/10/2009 and 20/09/2010, and each time we advect them with the flow using a sliding window of 10 days. The following image shows the amount of particles leaving the domain with time, and based on it we have fixed 3 days for the time-length for our trajectories. Choice of Δt: 6 hours choice is related to L2 products. Configuration to generate density maps: To generate the ensemble of density maps we used the following configuration: n_samples : 10000 ensemble_size : 2000 ensemble_radius : 5 km particle_runtime : 3 days particle_output_dt : 6 hours particle_dt_RK4 : dt(hour:=3) The following image shows an example of a sample:"
  },"/mldrop-pages/mldrop/2001-02-03-context.html": {
    "title": "A bit of Context",
    "keywords": "mldrop",
    "url": "/mldrop-pages/mldrop/2001-02-03-context.html",
    "body": "Observing the ocean’s circulation is an important issue for understanding the climate system and marine ecosystems. Ocean currents transport and redistribute heat, carbon, energy, and nutrients through the oceans. Over the past 25 years, the measurement of sea surface height obtained by satellite altimetry combined with the geostrophic approximation has made it possible to estimate largescale surface ocean circulation. However, these data don’t give any dynamics below 100km, which are very important in understanding the climate machinery in different oceanic applications. On the other hand, there exist other satellite images with higher spatial and temporal resolutions, particularly Sea Surface Temperature and Chlorophyll-a concentration, which in addition to their acquired quantities, they do encode information about other dynamics, particularly transport. This project takes a Lagrangian perspective and aims at taking full advantage of oceanic tracers to retrieve submesoscale transport properties, initially in the framework of search and rescue missions. Mathematically speaking, we aim at reconstructing trajectories of an object released in fluid flow. Taking a Lagrangian perspective, and knowing precisely the initial position of the object along with a good velocity field observation, this problem is straightforward to solve. Unfortunately, this is not always the case, the initial position does mostly include uncertainties along with the velocity fields observation that admit mapping and acquisition errors. This raises this deterministic problem into a stochastic one. Now we will have a cloud of trajectories instead of a single one: And these ensembles of trajectories can actually be modeled in the form of density maps as follow: In this project we are learning new machines that take as an input satellite tracers along with initial position of the lost object and give us as an output, time series of the probability density presence maps of the lost object."
  },"/mldrop-pages/zzcode/comments/": {
    "title": "Your comments",
    "keywords": "mldroppage",
    "url": "/mldrop-pages/zzcode/comments/",
    "body": "Your comments are welcome here :)"
  },"/mldrop-pages/zzcode/wikicode/": {
    "title": "Code wiki",
    "keywords": "mldroppage",
    "url": "/mldrop-pages/zzcode/wikicode/",
    "body": "Before starting, you will need to install the following packages, preferably on conda: Ocean Parcel Xarray Pytorch Torchvision Netcdf4 The tree of the project looks like: Project | ├── config │ └── data.py ├── core │ ├── data │ │ ├── datasets.py │ │ ├── generate.py │ │ └── process.py │ ├── model │ │ ├── __init__.py │ │ └── UNet │ │ ├── model.py │ │ └── parts.py │ ├── simulate │ │ ├── __init__.py │ │ └── trajectories │ │ ├── io.py │ │ ├── kernels.py │ │ └── particles.py │ └── util │ ├── geodesic.py │ ├── grid.py │ ├── __init__.py │ ├── misc.py │ ├── path.py │ └── random.py ├── density_maps.py ├── load.py ├── misc.py ├── README.md ├── snapshot.py └── test_model.py Use python density_maps.py to generate input data for training: training data | ├── density_maps ├── index_offsets.npy ├── initial_positions ├── tracers_fields ├── trajectories └── velocity_fields Edit misc.py to configure the input data, train/eval, directories.. and run python misc.py. Edit snapshot.py and run python snapshot \"name_of_model\" to start learning."
  }}
